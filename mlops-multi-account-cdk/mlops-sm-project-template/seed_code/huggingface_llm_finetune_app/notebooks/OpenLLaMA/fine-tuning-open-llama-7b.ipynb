{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285b8082-15ad-4ead-9ff0-c3bdba880787",
   "metadata": {},
   "source": [
    "## TODO: Check if same code betweem Falcon and LLama2 (and try to combine in 1 notebook) --> only difference in getting weights with LLama2 and huggingface token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe16a7-6a9c-4388-b1fe-1b28c2a44b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d36523-7de3-4211-86c2-45137f39c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9aa31-453a-48d8-a0b9-739e227d5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show torch\n",
    "!pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989007ab-29db-46ad-aa88-28c4bd364c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8f019-82f8-41b5-81a3-5532c82efd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer, \n",
    "    LlamaForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f923610-1ef8-476e-b168-df4aaec0ee30",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ebb15-dcfc-44f6-80bc-0cdd0eb63a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/example_qa_dataset.json\") as json_file: # Match this to your data path\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad1be5-226a-4ce9-b5af-adb9dd7fbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data[\"questions\"][0], sort_dicts=False)\n",
    "pprint(data[\"questions\"][1], sort_dicts=False)\n",
    "pprint(data[\"questions\"][2], sort_dicts=False)\n",
    "pprint(data[\"questions\"][3], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60156bd-6cf0-4e47-8e14-3788dbec4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset.json\", \"w\") as f:\n",
    "    json.dump(data[\"questions\"], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2b346-207a-443d-9519-c701789f3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data[\"questions\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5125b70-563d-4694-a0d3-07a5af687e74",
   "metadata": {},
   "source": [
    "## Load OpenLLaMA Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c876a-ea9f-47b3-88bb-d59fd8b7b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded and trained your LLaMA model\n",
    "model_name = 'openlm-research/open_llama_7b'\n",
    "\n",
    "# Specify the directory where you want to save the weights\n",
    "offload_folder= '../OpenLLaMA/open_llama_7b'\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(offload_folder)\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16, \n",
    "#     device_map='auto',\n",
    "#     offload_folder=offload_folder\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994c0aa-e322-49ec-9a16-f2cbfecc77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    offload_folder=offload_folder,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Tie the weights of the model\n",
    "model.tie_weights()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c24a0-8303-47da-b3d5-95738972dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    print(\"print_trainable_parameters start\")\n",
    "    \"\"\"\n",
    "    print the number of trainable parameters in the model\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            \n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319263b-2504-4509-9b2a-ebcdb8a0c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e12ee-e0a5-41bb-8f76-1377f706efcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47519ac-e709-4f08-8bd6-d02aa35bec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CASUAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096000e-b63d-4362-839d-987b3e175eb6",
   "metadata": {},
   "source": [
    "## Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff217f3-2ace-46de-a873-b051db17d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: How can I create an account?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804c253-e5f9-4048-8ee6-e4cf524f7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.8\n",
    "generation_config.top_p = 0.8\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229f7c6-be7f-4519-8c3c-a572e3e92b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = \"cuda:0\"\n",
    "# torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e543b8-7987-4376-86af-fed1057beb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to('cuda')\n",
    "print(next(model.parameters()).device)  # This will show the device of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31b258-2509-4327-990c-3bff6f386cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids = encoding.input_ids,\n",
    "        attention_mask = encoding.attention_mask,\n",
    "        generation_config = generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e61a-769f-4475-bd7f-265a5415d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference result before fine-tuning\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ef316-a157-4d84-aa8d-5f1f507a1b7b",
   "metadata": {},
   "source": [
    "## Build huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6252d-393e-445f-9987-b5b79a91aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=\"data/dataset.json\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69888a7e-494f-49e5-9824-0ce4ac14d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd31578-ee4f-4357-9976-bbb595662254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "    <human>: {data_point[\"question\"]}\n",
    "    <assistant>: {data_point[\"answer\"]}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    # tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    # print(tokenized_full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505e19a-6d08-4ee6-9629-dd21607fa4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae52cde-803e-4916-b7d1-c00f8a645812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the 'question', 'answer', 'token_type_ids' columns\n",
    "train_data = train_data.remove_columns(['question', 'answer',])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf38d37-6606-4f05-8ca5-1d78ad51f84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_data), type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9bcad-d456-42a8-b054-d9e559939653",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482947e8-7ca6-445d-845d-0a3b36a6e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f8ca4-debe-475f-b198-ac954a926a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e34624-ef7d-405a-842f-bf0b937dde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer and TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=5, \n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,  \n",
    "    max_steps = 60,\n",
    "    logging_steps=1,\n",
    "    output_dir=output_dir,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05, \n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_data,              \n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bef9b-f1dd-4a9e-9cd8-1bae50296466",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62d3cc-885c-44f3-9449-658a99b6e986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_model_dir = './ecommerce-FAQ-chatbot-model'\n",
    "model.save_pretrained(peft_model_dir)\n",
    "trainer.save_model(peft_model_dir)\n",
    "tokenizer.save_pretrained(peft_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789237d0-5982-4b29-ad69-d4d8cdbeb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub('seujeong/falcon-7b-glora-faq-chatbot', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d7d28-a201-44dc-ba43-cff2db0ad994",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "https://huggingface.co/blog/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816210f-0426-4688-ae5e-c5c27bd99312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a282d-b7ea-40bb-813a-9f8bfd2635f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "config = PeftConfig.from_pretrained(peft_model_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_dir)\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name).from_pretrained(config.base_model_name_or_path,  trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a705b8-a626-4dee-9cc4-d254858bec98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear the GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f95095-43ee-4fd0-b4ea-c52f2335c068",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d39e72-e1be-4660-b95e-f627f67de0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.8\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ca9fc-d27f-4903-91ea-95f252e8a73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c706bb3-8edd-4b71-84b1-3e8d21cf582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dfb7a-957e-48ff-a51a-212e5b159779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "<human>: {question}\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # model.to(device)  \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids.to('cuda'),\n",
    "            attention_mask=encoding.attention_mask.to('cuda'),\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    assistant_start = \"<assistant>:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    \n",
    "    if response_start >= 0:\n",
    "        # Find the second occurrence\n",
    "        responce_end = response.find(assistant_start, response_start + len(assistant_start))\n",
    "        # print(f\"'{assistant_start}' found in response: {responce_end}\")\n",
    "    else:\n",
    "        print(f\"'{assistant_start}' not found in response\")\n",
    "        \n",
    "    print(response)\n",
    "    print(\"------------------------------------------------------------------------------------\\n\")\n",
    "    return response[response_start+len(assistant_start):responce_end].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0515309-3a9e-42ec-ad62-f901f542e51b",
   "metadata": {},
   "source": [
    "### True dataset\n",
    "1. {'question': 'How can I create an account?',\n",
    " 'answer': \"To create an account, click on the 'Sign Up' button on the top \"\n",
    "           'right corner of our website and follow the instructions to '\n",
    "           'complete the registration process.'}\n",
    "           \n",
    "           \n",
    "2. {'question': 'What payment methods do you accept?',\n",
    " 'answer': 'We accept major credit cards, debit cards, and PayPal as payment '\n",
    "           'methods for online orders.'}\n",
    "           \n",
    "           \n",
    "3. {'question': 'How can I track my order?',\n",
    " 'answer': 'You can track your order by logging into your account and '\n",
    "           \"navigating to the 'Order History' section. There, you will find \"\n",
    "           'the tracking information for your shipment.'}\n",
    "           \n",
    "           \n",
    "4. {'question': 'What is your return policy?',\n",
    " 'answer': 'Our return policy allows you to return products within 30 days of '\n",
    "           'purchase for a full refund, provided they are in their original '\n",
    "           'condition and packaging. Please refer to our Returns page for '\n",
    "           'detailed instructions.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdb82d-7859-4b7d-8c9d-0f66fa55bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can I return a product if it was a clearance or final sale item?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6a8dd-d260-47af-b939-90a410c77548",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What happens when I return a clearance item?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642f0f-679f-413f-a3df-d584177d2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How do I know when I'll receive my order?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bcbec-57b8-4f4b-b24e-e056d5496db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Do you accept credit caards or paypal?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1160f91-eae2-45a7-a74c-8cb3e5b2f307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me how to make a new account\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b15e3-a740-4393-b88a-70a1b8831403",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want to track my order, can you tell me how to do?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1adad-57af-46bf-ba8b-3cb2c56274f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me the return policy\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607dd77-1dd8-4f64-9ce1-501e120f411a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generation_config = model.generation_config\n",
    "# generation_config.max_new_tokens = 100\n",
    "# generation_config.temperature = 0.7\n",
    "# generation_config.top_p = 0.9\n",
    "# generation_config.num_return_sequences = 1\n",
    "# generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "# generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa948f-7057-4e97-a265-d3219963c08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_prompts = [\"Can I return a product if it was a clearance or final sale item?\",\"What happens when I return a clearance item?\", \"How do I know when I'll receive my order?\", \"Do you accept credit caards or paypal?\", \"Tell me how to make a new account\"]\n",
    "\n",
    "# def test_generate_response(question: str) -> str:\n",
    "#     prompt = f\"\"\"\n",
    "# <human>: {question}\n",
    "# <assistant>:\n",
    "# \"\"\".strip()\n",
    "#     print(\"#########################################################################\")\n",
    "#     print(\"question: \", question)\n",
    "#     print(\"#########################################################################\")\n",
    "#     for p in range(1, 11):\n",
    "#         # generation_config.temperature = p\n",
    "#         p = round(p*0.1, 1)\n",
    "#         generation_config.top_p = p\n",
    "#         # print(\"---------------------------------------------------------------------------------\")\n",
    "#         print(f\"generation_config -> temperature is {generation_config.temperature}, top_p is {generation_config.top_p}\")\n",
    "\n",
    "#         encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#         # model.to(device)  \n",
    "#         with torch.inference_mode():\n",
    "#             outputs = model.generate(\n",
    "#                 input_ids=encoding.input_ids.to('cuda'),\n",
    "#                 attention_mask=encoding.attention_mask.to('cuda'),\n",
    "#                 generation_config=generation_config,\n",
    "#             )\n",
    "#         response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         assistant_start = \"<assistant>:\"\n",
    "#         response_start = response.find(assistant_start)\n",
    "\n",
    "#         if response_start >= 0:\n",
    "#             # Find the second occurrence\n",
    "#             responce_end = response.find(assistant_start, response_start + len(assistant_start))\n",
    "#             # print(f\"'{assistant_start}' found in response: {responce_end}\")\n",
    "#         else:\n",
    "#             print(f\"'{assistant_start}' not found in response\")\n",
    "\n",
    "#         # print(\"response start------------------------------------------\")\n",
    "#         # print(response)\n",
    "#         # print(\"response end------------------------------------------\")\n",
    "#         print(f\"response_start: {response_start},\\n Final answer: {response[response_start+len(assistant_start):responce_end]}\")\n",
    "#         # print(\"--------------------------------Return--------------------------------\")\n",
    "#     return response[response_start+len(assistant_start):responce_end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb7a61-f4b5-4434-9d5a-00a44801aef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for prompt in test_prompts:\n",
    "#     print(test_generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3d919-02f7-44c3-a2b9-270a23fff4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# generation_config.top_p = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f7938-1fd1-41ad-80d7-05c3f629e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompts = [\"Can I return a product if it was a clearance or final sale item?\",\"What happens when I return a clearance item?\", \"How do I know when I'll receive my order?\", \"Do you accept credit caards or paypal?\", \"Tell me how to make a new account\"]\n",
    "\n",
    "# def test_generate_response(question: str) -> str:\n",
    "#     prompt = f\"\"\"\n",
    "# <human>: {question}\n",
    "# <assistant>:\n",
    "# \"\"\".strip()\n",
    "#     print(\"#########################################################################\")\n",
    "#     print(\"question: \", question)\n",
    "#     print(\"#########################################################################\")\n",
    "#     for p in range(1, 11):\n",
    "#         # generation_config.temperature = p\n",
    "#         p = round(p*0.1, 1)\n",
    "#         generation_config.temperature = p\n",
    "#         # print(\"---------------------------------------------------------------------------------\")\n",
    "#         print(f\"generation_config -> top_p is {generation_config.top_p}, temperature is {generation_config.temperature}\")\n",
    "\n",
    "#         encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#         # model.to(device)  \n",
    "#         with torch.inference_mode():\n",
    "#             outputs = model.generate(\n",
    "#                 input_ids=encoding.input_ids.to('cuda'),\n",
    "#                 attention_mask=encoding.attention_mask.to('cuda'),\n",
    "#                 generation_config=generation_config,\n",
    "#             )\n",
    "#         response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         assistant_start = \"<assistant>:\"\n",
    "#         response_start = response.find(assistant_start)\n",
    "\n",
    "#         if response_start >= 0:\n",
    "#             # Find the second occurrence\n",
    "#             responce_end = response.find(assistant_start, response_start + len(assistant_start))\n",
    "#             # print(f\"'{assistant_start}' found in response: {responce_end}\")\n",
    "#         else:\n",
    "#             print(f\"'{assistant_start}' not found in response\")\n",
    "\n",
    "#         # print(\"response start------------------------------------------\")\n",
    "#         # print(response)\n",
    "#         # print(\"response end------------------------------------------\")\n",
    "#         print(f\"Final answer: {response[response_start+len(assistant_start):responce_end]}\")\n",
    "#         # print(\"--------------------------------Return--------------------------------\")\n",
    "#     return response[response_start+len(assistant_start):responce_end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb89381-8848-48eb-a966-f32f4a2a0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prompt in test_prompts:\n",
    "#     print(test_generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03778109-5e93-4776-8820-ea8a721d1b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
