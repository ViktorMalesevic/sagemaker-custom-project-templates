{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2be69a-bfe7-430c-b5bb-31ec3f5e85b7",
   "metadata": {},
   "source": [
    "# Fine tuning Falcon 7B model with Ecommerce FAQ dataset\n",
    "\n",
    "PEFT : Parameter Efficienct Fine Tuning\n",
    "* In short, PEFT approaches enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.\n",
    "* https://huggingface.co/blog/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61e668-9702-4507-9e48-04228f28f043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "# !pip install -qqq torch --progress-bar off\n",
    "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e303a9cc --progress-bar off\n",
    "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\n",
    "# !pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\n",
    "!pip install -qqq datasets loralib accelerate einops ipywidgets --progress-bar off\n",
    "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
    "!pip install -Uqqq accelerate --progress-bar off\n",
    "!pip install -Uqqq deepspeed transformers --progress-bar off\n",
    "!pip install -qqq tensorboard --progress-bar off\n",
    "!pip install -qqq optuna bert_score evaluate --progress-bar off\n",
    "!pip install -Uq bitsandbytes --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d36523-7de3-4211-86c2-45137f39c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9aa31-453a-48d8-a0b9-739e227d5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show torch\n",
    "!pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989007ab-29db-46ad-aa88-28c4bd364c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8f019-82f8-41b5-81a3-5532c82efd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from evaluate import load\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from bert_score import score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "# Filter specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings('ignore', message=\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\")\n",
    "warnings.filterwarnings('ignore', message=\"You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f923610-1ef8-476e-b168-df4aaec0ee30",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ebb15-dcfc-44f6-80bc-0cdd0eb63a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Ecommerce_FAQ_Chatbot_dataset.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad1be5-226a-4ce9-b5af-adb9dd7fbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(data[\"questions\"][0], sort_dicts=False)\n",
    "# pprint(data[\"questions\"][1], sort_dicts=False)\n",
    "# pprint(data[\"questions\"][2], sort_dicts=False)\n",
    "# pprint(data[\"questions\"][3], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60156bd-6cf0-4e47-8e14-3788dbec4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset.json\", \"w\") as f:\n",
    "    json.dump(data[\"questions\"], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2b346-207a-443d-9519-c701789f3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data[\"questions\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5125b70-563d-4694-a0d3-07a5af687e74",
   "metadata": {},
   "source": [
    "## Load Falcon Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65322b6f-b9d6-4363-b834-d1a31810fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(model_name):\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "   \n",
    "    # tokenizer.pad_token  = tokenizer.eos_token \n",
    "    \n",
    "#     additional_tokens = \"<|pad|>\"\n",
    "#     num_added_toks = tokenizer.add_tokens(additional_tokens)\n",
    "#     print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "#     model.resize_token_embeddings(len(tokenizer))  # Resize the model vocabulary\n",
    "#     tokenizer.pad_token = \"<|pad|>\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CASUAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "model, tokenizer = get_model_tokenizer(model_name)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994c0aa-e322-49ec-9a16-f2cbfecc77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=bnb_config,\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c24a0-8303-47da-b3d5-95738972dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    print the number of trainable parameters in the model\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            \n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319263b-2504-4509-9b2a-ebcdb8a0c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5827b77-e39a-436a-80e9-3a49acea2475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096000e-b63d-4362-839d-987b3e175eb6",
   "metadata": {},
   "source": [
    "## Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff217f3-2ace-46de-a873-b051db17d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "<human>: How can I create an account?/n\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804c253-e5f9-4048-8ee6-e4cf524f7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 100\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229f7c6-be7f-4519-8c3c-a572e3e92b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = \"cuda:0\"\n",
    "# torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e543b8-7987-4376-86af-fed1057beb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)  # This will show the device of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31b258-2509-4327-990c-3bff6f386cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids = encoding.input_ids,\n",
    "        attention_mask = encoding.attention_mask,\n",
    "        generation_config = generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e61a-769f-4475-bd7f-265a5415d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference result before fine-tuning\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92e67a-87ae-4ef1-bd4b-d020c6dae58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What are the steps to create an account?\",  # Original: 'How can I create an account?'\n",
    "    \"Which types of payment can I use?\",  # Original: 'What payment methods do you accept?'\n",
    "    \"How do I monitor the status of my order?\",  # Original: 'How can I track my order?'\n",
    "    \"Can you describe your policy on returns?\",  # Original: 'What is your return policy?'\n",
    "    \"Is it possible to return an item that was bought during a final sale or clearance?\",  # Original: 'Can I return a product if it was a clearance or final sale item?'\n",
    "    \"What happend if I return clearance item?\"\n",
    "]\n",
    "\n",
    "for question in eval_questions:\n",
    "    prompt = f\"\"\"\n",
    "    User: {question}\n",
    "    AI: \n",
    "    \"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids = encoding.input_ids,\n",
    "            attention_mask = encoding.attention_mask,\n",
    "            generation_config = generation_config,\n",
    "        )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ef316-a157-4d84-aa8d-5f1f507a1b7b",
   "metadata": {},
   "source": [
    "## Build huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6252d-393e-445f-9987-b5b79a91aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=\"data/dataset.json\")\n",
    "# data = data[\"questions\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69888a7e-494f-49e5-9824-0ce4ac14d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd31578-ee4f-4357-9976-bbb595662254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "    User: {data_point[\"question\"]}\n",
    "    AI: {data_point[\"answer\"]}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    # tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    # print(tokenized_full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505e19a-6d08-4ee6-9629-dd21607fa4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae52cde-803e-4916-b7d1-c00f8a645812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the 'question', 'answer', 'token_type_ids' columns\n",
    "train_data = train_data.remove_columns(['question', 'answer', 'token_type_ids'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf38d37-6606-4f05-8ca5-1d78ad51f84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_data), type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781b0e5-1934-42db-8eff-3d1ff833a922",
   "metadata": {},
   "source": [
    "## HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482947e8-7ca6-445d-845d-0a3b36a6e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e461c48-25d5-413f-b98e-2b1e86142a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What are the steps to create an account?\",  # Original: 'How can I create an account?'\n",
    "    \"Which types of payment can I use?\",  # Original: 'What payment methods do you accept?'\n",
    "    \"How do I monitor the status of my order?\",  # Original: 'How can I track my order?'\n",
    "    \"Can you describe your policy on returns?\",  # Original: 'What is your return policy?'\n",
    "    \"Is it possible to return an item that was bought during a final sale or clearance?\",  # Original: 'Can I return a product if it was a clearance or final sale item?'\n",
    "    \"What happend if I return clearance item?\"\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\",\n",
    "    \"We accept major credit cards, debit cards, and PayPal as payment methods for online orders.\",\n",
    "    \"You can track your order by logging into your account and navigating to the 'Order History' section. There, you will find the tracking information for your shipment.\",\n",
    "    \"Our return policy allows you to return products within 30 days of purchase for a full refund, provided they are in their original condition and packaging. Please refer to our Returns page for detailed instructions.\",\n",
    "    \"Clearance or final sale items are typically non-returnable and non-refundable. Please review the product description or contact our customer support team for more information.\",\n",
    "    \"Returning clearance items is generally not possible. Please check the product description or contact our customer support team for more information. Please note that clearance items are typically final sale and cannot be returned or exchanged. I hope this helps! If you have any additional questions, please feel free to contact our customer support team. We are happy to assist you.\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed02c2-14cf-4e2a-af7a-f9005c7aab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_data(prompt, model, tokenizer):\n",
    "   \n",
    "    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    print(f\"Question for the inference:\\n{prompt}\")\n",
    "    \n",
    "    # encoding = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    encoding = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "        \n",
    "    print(model.parameters().__next__().device)\n",
    "    model.config.gradient_checkpointing = False\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids = encoding.input_ids.requires_grad_(False),\n",
    "            attention_mask = encoding.attention_mask.requires_grad_(False),\n",
    "            generation_config = generation_config,\n",
    "        )\n",
    "    # inference result before fine-tuning\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "def bertscore_metrics(eval_questions, eval_answers, model, tokenizer):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(eval_questions)):\n",
    "        prediction = inference_data(eval_questions[i], model, tokenizer)\n",
    "        label = eval_answers[i]\n",
    "        print(f\"----------------------------predictions----------------------------\\n{prediction}\")\n",
    "        lines = prediction.split('\\n')\n",
    "        print(lines)\n",
    "        print(f\"----------------------------answers----------------------------\\n{label}\")\n",
    "\n",
    "        \n",
    "        predictions.append(prediction.strip().lower())\n",
    "        labels.append(label.strip().lower())\n",
    "\n",
    "    bertscore = load(\"bertscore\")\n",
    "    results = bertscore.compute(predictions=predictions, references=labels, model_type=\"distilbert-base-uncased\")\n",
    "    # print(\"results in bertscore_batch_metrics\",results)\n",
    "    precisions, recalls, f1_scores = results['precision'], results['recall'], results['f1']\n",
    "    avg_precision, avg_recall, avg_f1_score =(sum(precisions) / len(precisions)),( sum(recalls) / len(recalls)), ( sum(f1_scores) / len(f1_scores))\n",
    "    avg_results = {'precision':avg_precision, 'recall':avg_recall, 'f1':avg_f1_score}\n",
    "  \n",
    "    print(\"avg_results\", avg_results)\n",
    "    return avg_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51931c02-44ae-4c1b-b5dc-9e7b4174ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 10)\n",
    "    max_steps = trial.suggest_categorical(\"max_steps\", list(range(80,241,20)))\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [1, 2, 4])\n",
    "    # warmup_steps = trial.suggest_categorical(\"warmup_steps\", list(range(50,301,50)))\n",
    "    # per_device_train_batch_size = 1\n",
    "    \n",
    "    print(\"######################################################################################################################\")\n",
    "    print(f\"[{trial.number+1}/100] --- learning_rate:{learning_rate} | num_train_epochs:{num_train_epochs} | max_steps:{max_steps} | per_device_train_batch_size:{per_device_train_batch_size}\")\n",
    "    # Use hyperparameters in TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        warmup_ratio=0.1,\n",
    "        max_steps = max_steps,\n",
    "        # Other fixed parameters\n",
    "        remove_unused_columns=False,\n",
    "        fp16=False,\n",
    "        save_total_limit=3,  \n",
    "        logging_steps=10,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        # report_to=\"tensorboard\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_name = \"tiiuae/falcon-7b\"\n",
    "    new_model, new_tokenizer = get_model_tokenizer(model_name)\n",
    "\n",
    "    model.config.gradient_checkpointing = False\n",
    "    print(model.parameters().__next__().device)\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=new_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(new_tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.config.use_cache = False  # Disable caching\n",
    "    trainer.train()\n",
    "    print(\"after training\")\n",
    " \n",
    "    avg_results = bertscore_metrics(eval_questions, eval_answers, new_model, new_tokenizer)\n",
    "    print(avg_results)  \n",
    "    \n",
    "    return avg_results['f1']    \n",
    "\n",
    "# Create a study to run hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbd161-9ae6-46bf-b1dc-f0e7696a5bf6",
   "metadata": {},
   "source": [
    "## Training with best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea4b5b-0fff-4dc1-aed0-01848bcd53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best Trial: score {best_trial.value}, params {best_trial.params}\")\n",
    "\n",
    "best_score = best_trial.value\n",
    "if best_score < 0.8:\n",
    "    \n",
    "    print(f'Best score is {best_trial.value} < 0.8 !!!!! Start the loop')\n",
    "    while best_trial.value < 0.8:\n",
    "        # Create a study to run hyperparameter optimization\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "        best_trial = study.best_trial\n",
    "        best_score = best_trial.value\n",
    "\n",
    "print(f'Best score is {best_trial.value}!!!!!')\n",
    "\n",
    "best_params = best_trial.params\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "model, tokenizer = get_model_tokenizer(model_name)\n",
    "\n",
    "model.config.gradient_checkpointing = False\n",
    "print(model.parameters().__next__().device)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "    num_train_epochs=best_params['num_train_epochs'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps = best_params['max_steps'],\n",
    "    # Other fixed parameters\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,\n",
    "    save_total_limit=3,  \n",
    "    logging_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "peft_model_dir = './ecommerce-FAQ-chatbot-model/Optuna'\n",
    "model.save_pretrained(peft_model_dir)\n",
    "trainer.save_model(peft_model_dir)\n",
    "tokenizer.save_pretrained(peft_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e094513-ed12-4dfd-b208-fdc380012b69",
   "metadata": {},
   "source": [
    "### Load fine-tuned model and tokenizer\n",
    "\n",
    "https://huggingface.co/blog/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f4a9c-91db-4a44-be78-49a4b0658d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "config = PeftConfig.from_pretrained(peft_model_dir)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_dir)\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,  trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc29386-858b-4bf2-8ad8-4db9715cdb7a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### True dataset\n",
    "1. {'question': 'How can I create an account?',\n",
    " 'answer': \"To create an account, click on the 'Sign Up' button on the top \"\n",
    "           'right corner of our website and follow the instructions to '\n",
    "           'complete the registration process.'}\n",
    "           \n",
    "           \n",
    "2. {'question': 'What payment methods do you accept?',\n",
    " 'answer': 'We accept major credit cards, debit cards, and PayPal as payment '\n",
    "           'methods for online orders.'}\n",
    "           \n",
    "           \n",
    "3. {'question': 'How can I track my order?',\n",
    " 'answer': 'You can track your order by logging into your account and '\n",
    "           \"navigating to the 'Order History' section. There, you will find \"\n",
    "           'the tracking information for your shipment.'}\n",
    "           \n",
    "                    \n",
    "4. {'question':'What is your return policy?'\n",
    "'answer':'Our return policy allows you to return products within 30 days of purchase for a full refund, provided they are in their original condition and packaging. Please refer to our Returns page for detailed instructions.'}\n",
    "\n",
    "\n",
    "5. {'question':'Can I return a product if it was a clearance or final sale item?'\n",
    "'answer':'Clearance or final sale items are typically non-returnable and non-refundable. Please review the product description or contact our customer support team for more information.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ede13-b9be-429f-bebc-828e7585870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What are the steps to establish an account?\",  # Original: 'How can I create an account?'\n",
    "    \"Which types of payment can I use?\",  # Original: 'What payment methods do you accept?'\n",
    "    \"How do I monitor the status of my order?\",  # Original: 'How can I track my order?'\n",
    "    \"Can you describe your policy on returns?\",  # Original: 'What is your return policy?'\n",
    "    \"Is it possible to return an item that was bought during a final sale or clearance?\",  # Original: 'Can I return a product if it was a clearance or final sale item?'\n",
    "    \"What happend if I return clearance item?\"\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\",\n",
    "    \"We accept major credit cards, debit cards, and PayPal as payment methods for online orders.\",\n",
    "    \"You can track your order by logging into your account and navigating to the 'Order History' section. There, you will find the tracking information for your shipment.\",\n",
    "    \"Our return policy allows you to return products within 30 days of purchase for a full refund, provided they are in their original condition and packaging. Please refer to our Returns page for detailed instructions.\",\n",
    "    \"Clearance or final sale items are typically non-returnable and non-refundable. Please review the product description or contact our customer support team for more information.\",\n",
    "    \"Clearance or final sale items are typically non-returnable and non-refundable. Please review the product description or contact our customer support team for more information.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e361976-f017-466e-8797-e8bf3fa8e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results = bertscore_metrics(eval_questions, eval_answers, model, tokenizer)\n",
    "print(avg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a705b8-a626-4dee-9cc4-d254858bec98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear the GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f95095-43ee-4fd0-b4ea-c52f2335c068",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d39e72-e1be-4660-b95e-f627f67de0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 100\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ca9fc-d27f-4903-91ea-95f252e8a73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c706bb3-8edd-4b71-84b1-3e8d21cf582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dfb7a-957e-48ff-a51a-212e5b159779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_at_last_comma(text):\n",
    "    # Find the last occurrence of a comma in the text\n",
    "    last_comma_index = text.rfind('.')\n",
    "\n",
    "    # If a comma is found, cut the text up to the character after the last comma\n",
    "    if last_comma_index != -1:\n",
    "        return text[:last_comma_index + 1]\n",
    "    \n",
    "    # If no comma is found, return the original text\n",
    "    return text\n",
    "\n",
    "def post_processing_response(question, model, tokenizer):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        User: {question}\n",
    "        AI:\n",
    "        \"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # model.to(device)  \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids.to('cuda'),\n",
    "            attention_mask=encoding.attention_mask.to('cuda'),\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    assistant_start = \"AI:\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    \n",
    "    if response_start >= 0:\n",
    "        # Find the second occurrence\n",
    "        responce_end = response.find(assistant_start, response_start + len(assistant_start))\n",
    "        # print(f\"'{assistant_start}' found in response: {responce_end}\")\n",
    "    else:\n",
    "        print(f\"'{assistant_start}' not found in response\")\n",
    "        \n",
    "    print(response)\n",
    "    print(\"------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    processed_response = response[response_start+len(assistant_start):responce_end].strip()\n",
    "    processed_response = cut_at_last_comma(processed_response)\n",
    "    \n",
    "    return processed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0515309-3a9e-42ec-ad62-f901f542e51b",
   "metadata": {},
   "source": [
    "### True dataset\n",
    "1. {'question': 'How can I create an account?',\n",
    " 'answer': \"To create an account, click on the 'Sign Up' button on the top \"\n",
    "           'right corner of our website and follow the instructions to '\n",
    "           'complete the registration process.'}\n",
    "           \n",
    "           \n",
    "2. {'question': 'What payment methods do you accept?',\n",
    " 'answer': 'We accept major credit cards, debit cards, and PayPal as payment '\n",
    "           'methods for online orders.'}\n",
    "           \n",
    "           \n",
    "3. {'question': 'How can I track my order?',\n",
    " 'answer': 'You can track your order by logging into your account and '\n",
    "           \"navigating to the 'Order History' section. There, you will find \"\n",
    "           'the tracking information for your shipment.'}\n",
    "           \n",
    "           \n",
    "4. {'question': 'What is your return policy?',\n",
    " 'answer': 'Our return policy allows you to return products within 30 days of '\n",
    "           'purchase for a full refund, provided they are in their original '\n",
    "           'condition and packaging. Please refer to our Returns page for '\n",
    "           'detailed instructions.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdb82d-7859-4b7d-8c9d-0f66fa55bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can I return a product if it was a clearance or final sale item?\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6a8dd-d260-47af-b939-90a410c77548",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What happens when I return a clearance item?\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642f0f-679f-413f-a3df-d584177d2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How do I know when I'll receive my order?\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bcbec-57b8-4f4b-b24e-e056d5496db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Do you accept credit caards or paypal?\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1160f91-eae2-45a7-a74c-8cb3e5b2f307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me how to make a new account\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33777c95-a144-4891-931c-981f2e2e2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want to track my order, can you tell me how to do?\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd12f27-2d86-44f4-b6ff-715d2592e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me the return policy\"\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3165e67-718c-4b83-a453-1ebf62a935c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare the zero-shot results and fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb89381-8848-48eb-a966-f32f4a2a0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "ori_model, ori_tokenizer = get_model_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d53d0-19e5-42c5-80d8-5649d3aa8823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Can I return a product if it was a clearance or final sale item?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b41a4-3669-463f-97bb-738313dc2812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(post_processing_response(prompt, ori_model, ori_tokenizer))\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0150ae0-29a8-4cd6-918b-4e189291910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What happens when I return a clearance item?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b368b-5a4c-45b3-bf96-ce17030f998a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(post_processing_response(prompt, ori_model, ori_tokenizer))\n",
    "print(post_processing_response(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10f321-1ae4-4e97-83b3-acab25a65a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "ori_model, ori_tokenizer = get_model_tokenizer(model_name)\n",
    "\n",
    "for question in eval_questions:\n",
    "    print(\"Zero-Shot results:\\n\")\n",
    "    print(post_processing_response(question, ori_model, ori_tokenizer))\n",
    "    print(\"###############################################################\")\n",
    "    print(\"Fine-tuned results:\\n\")\n",
    "    print(post_processing_response(question, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
